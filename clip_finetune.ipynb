{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any, Optional\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPClassifier:\n",
    "    def __init__(self, model_name: str = \"openai/clip-vit-base-patch32\"):\n",
    "        \"\"\"\n",
    "        Initialize the CLIP classifier\n",
    "        Args:\n",
    "            model_name: Name of the CLIP model to use\n",
    "        \"\"\"\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        self.model = CLIPModel.from_pretrained(model_name).to(self.device)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "        \n",
    "    def prepare_dataset(self, dataset_name: str, split: str = \"train\", \n",
    "                       text_column: str = \"text\", image_column: str = \"image\", **kwargs):\n",
    "        \"\"\"\n",
    "        Load and prepare the dataset from Hugging Face\n",
    "        \"\"\"\n",
    "        self.dataset = load_dataset(dataset_name, split=split, **kwargs)\n",
    "        self.text_column = text_column\n",
    "        self.image_column = image_column\n",
    "        \n",
    "        # Get unique labels\n",
    "        self.labels = sorted(list(set(self.dataset[text_column])))\n",
    "        print(f\"Found {len(self.labels)} unique labels: {self.labels}\")\n",
    "        \n",
    "        # Generate CLIP-style label descriptions\n",
    "        self.clip_labels = [f\"a photo of a {label}\" for label in self.labels]\n",
    "        \n",
    "        # Create and process label tokens\n",
    "        self.label_tokens = self.processor(\n",
    "            text=self.clip_labels,\n",
    "            padding=True,\n",
    "            images=None,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Generate label embeddings\n",
    "        with torch.no_grad():\n",
    "            self.label_embeddings = self.model.get_text_features(**self.label_tokens)\n",
    "            self.label_embeddings = self.label_embeddings.detach().cpu().numpy()\n",
    "            # Normalize label embeddings\n",
    "            self.label_embeddings = self.label_embeddings / np.linalg.norm(self.label_embeddings, axis=0)\n",
    "\n",
    "    def inference_on_samples(self, sample_indices: List[int]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform inference on specific samples\n",
    "        Args:\n",
    "            sample_indices: List of indices to perform inference on\n",
    "        Returns:\n",
    "            Dictionary containing predictions and confidence scores\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for idx in sample_indices:\n",
    "            sample = self.dataset[idx]\n",
    "            \n",
    "            # Process image\n",
    "            image = self.processor(\n",
    "                text=None,\n",
    "                images=sample[self.image_column],\n",
    "                return_tensors='pt'\n",
    "            )['pixel_values'].to(self.device)\n",
    "            \n",
    "            # Get image embeddings\n",
    "            with torch.no_grad():\n",
    "                image_embeddings = self.model.get_image_features(image)\n",
    "                image_embeddings = image_embeddings.detach().cpu().numpy()\n",
    "            \n",
    "            # Calculate similarity scores\n",
    "            scores = np.dot(image_embeddings, self.label_embeddings.T)[0]\n",
    "            pred_idx = np.argmax(scores)\n",
    "            confidence_scores = softmax(scores)\n",
    "            \n",
    "            results.append({\n",
    "                'index': idx,\n",
    "                'true_label': sample[self.text_column],\n",
    "                'predicted_label': self.labels[pred_idx],\n",
    "                'confidence': confidence_scores[pred_idx],\n",
    "                'all_scores': dict(zip(self.labels, confidence_scores))\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def plot_confusion_matrix(self, true_labels: List[str], pred_labels: List[str], \n",
    "                            output_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Plot confusion matrix\n",
    "        \"\"\"\n",
    "        cm = confusion_matrix(true_labels, pred_labels, labels=self.labels)\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=self.labels, yticklabels=self.labels)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if output_path:\n",
    "            plt.savefig(output_path)\n",
    "        plt.close()\n",
    "\n",
    "    def plot_metrics_comparison(self, metrics_dict: Dict[str, float], \n",
    "                              output_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Plot comparison of different metrics\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(metrics_dict.keys(), metrics_dict.values())\n",
    "        plt.title('Classification Metrics Comparison')\n",
    "        plt.ylabel('Score')\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.ylim(0, 1.1)  # Set y-axis limit to 0-1 with some padding\n",
    "        if output_path:\n",
    "            plt.savefig(output_path)\n",
    "        plt.close()\n",
    "\n",
    "    def evaluate(self, batch_size: int = 32) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform evaluation with multiple metrics\n",
    "        \"\"\"\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        all_scores = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(self.dataset), batch_size)):\n",
    "            i_end = min(i + batch_size, len(self.dataset))\n",
    "            batch = self.dataset[i:i_end]\n",
    "            \n",
    "            # Process batch of images\n",
    "            images = self.processor(\n",
    "                text=None,\n",
    "                images=batch[self.image_column],\n",
    "                return_tensors='pt'\n",
    "            )['pixel_values'].to(self.device)\n",
    "            \n",
    "            # Get image embeddings\n",
    "            with torch.no_grad():\n",
    "                image_embeddings = self.model.get_image_features(images)\n",
    "                image_embeddings = image_embeddings.detach().cpu().numpy()\n",
    "            \n",
    "            # Calculate similarity scores\n",
    "            scores = np.dot(image_embeddings, self.label_embeddings.T)\n",
    "            predictions = np.argmax(scores, axis=1)\n",
    "            \n",
    "            # Store predictions and true labels\n",
    "            all_predictions.extend([self.labels[idx] for idx in predictions])\n",
    "            all_labels.extend(batch[self.text_column])\n",
    "            all_scores.extend(scores)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            all_labels, all_predictions, average='weighted'\n",
    "        )\n",
    "        \n",
    "        # Create classification report\n",
    "        report = classification_report(\n",
    "            all_labels, all_predictions, \n",
    "            target_names=self.labels, \n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n",
    "        \n",
    "        # Compile metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "        \n",
    "        # Generate plots\n",
    "        self.plot_confusion_matrix(all_labels, all_predictions, 'confusion_matrix.png')\n",
    "        self.plot_metrics_comparison(metrics, 'metrics_comparison.png')\n",
    "        \n",
    "        return {\n",
    "            'metrics': metrics,\n",
    "            'detailed_report': report,\n",
    "            'predictions': all_predictions,\n",
    "            'true_labels': all_labels,\n",
    "            'raw_scores': all_scores\n",
    "        }\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax values for each set of scores\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Found 2 unique labels: [0, 1]\n",
      "\n",
      "Performing evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:40<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Metrics:\n",
      "accuracy: 0.3833\n",
      "precision: 0.3672\n",
      "recall: 0.3833\n",
      "f1_score: 0.3640\n",
      "\n",
      "Performing inference on sample images...\n",
      "\n",
      "Sample Predictions:\n",
      "\n",
      "Image 0:\n",
      "True label: 0\n",
      "Predicted label: 0\n",
      "Confidence: 0.5803\n",
      "\n",
      "Image 10:\n",
      "True label: 0\n",
      "Predicted label: 0\n",
      "Confidence: 0.9882\n",
      "\n",
      "Image 20:\n",
      "True label: 0\n",
      "Predicted label: 1\n",
      "Confidence: 0.9987\n",
      "\n",
      "Image 30:\n",
      "True label: 0\n",
      "Predicted label: 1\n",
      "Confidence: 0.5716\n",
      "\n",
      "Image 40:\n",
      "True label: 0\n",
      "Predicted label: 0\n",
      "Confidence: 0.9184\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Initialize classifier\n",
    "    classifier = CLIPClassifier()\n",
    "    \n",
    "    # Prepare dataset\n",
    "    classifier.prepare_dataset(\n",
    "        \"dataset\",\n",
    "        split=\"train\",\n",
    "        text_column=\"label\",\n",
    "        image_column=\"image\"\n",
    "    )\n",
    "    \n",
    "    # Perform evaluation\n",
    "    print(\"\\nPerforming evaluation...\")\n",
    "    eval_results = classifier.evaluate(batch_size=32)\n",
    "    \n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    for metric, value in eval_results['metrics'].items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Perform inference on some samples\n",
    "    print(\"\\nPerforming inference on sample images...\")\n",
    "    sample_indices = [0, 10, 20, 30, 40]  \n",
    "    sample_results = classifier.inference_on_samples(sample_indices)\n",
    "    \n",
    "    print(\"\\nSample Predictions:\")\n",
    "    for result in sample_results:\n",
    "        print(f\"\\nImage {result['index']}:\")\n",
    "        print(f\"True label: {result['true_label']}\")\n",
    "        print(f\"Predicted label: {result['predicted_label']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_implementations_comparison(implementations_results: Dict[str, Dict[str, float]], \n",
    "                                   output_path: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Create a line plot comparing metrics between different implementations\n",
    "    Args:\n",
    "        implementations_results: Dictionary containing results from different implementations\n",
    "        output_path: Optional path to save the plot\n",
    "    \"\"\"\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    metric_keys = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    \n",
    "    # Set style parameters\n",
    "    plt.style.use('bmh')  # Using 'bmh' style which provides good visualization for data comparison\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot lines for each implementation\n",
    "    markers = ['o', 's', 'D', '^', 'v']  # Different markers for different implementations\n",
    "    for idx, (impl_name, results) in enumerate(implementations_results.items()):\n",
    "        values = [results[key] for key in metric_keys]\n",
    "        plt.plot(metrics, values, marker=markers[idx], label=impl_name, linewidth=2, markersize=8)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Score', fontsize=12, fontweight='bold')\n",
    "    plt.title('Comparison of Different Model Implementations for Autism Detection', fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Enhance grid and spines\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Customize legend\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', frameon=True, fancybox=True, shadow=True)\n",
    "    \n",
    "    # Customize tick labels\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    \n",
    "    # Set y-axis limits with some padding\n",
    "    plt.ylim(0, 1.1)\n",
    "    \n",
    "    # Add value annotations\n",
    "    for impl_name, results in implementations_results.items():\n",
    "        values = [results[key] for key in metric_keys]\n",
    "        for i, value in enumerate(values):\n",
    "            plt.annotate(f'{value:.3f}', \n",
    "                        (metrics[i], value),\n",
    "                        textcoords=\"offset points\",\n",
    "                        xytext=(0,10),\n",
    "                        ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "        print(f\"Plot saved to {output_path}\")\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "def compare_implementations(implementations_results: Dict[str, Dict[str, float]]) -> Dict:\n",
    "    \"\"\"\n",
    "    Compare metrics between different implementations\n",
    "    Args:\n",
    "        implementations_results: Dictionary containing results from different implementations\n",
    "        Example format:\n",
    "        {\n",
    "            'CLIP Custom': {'accuracy': 0.87, 'precision': 0.86, 'recall': 0.85, 'f1_score': 0.86},\n",
    "            'PyTorch CLIP': {'accuracy': 0.85, 'precision': 0.84, 'recall': 0.83, 'f1_score': 0.83},\n",
    "            ...\n",
    "        }\n",
    "    Returns:\n",
    "        Dictionary with formatted comparison data\n",
    "    \"\"\"\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    metric_keys = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    \n",
    "    comparison_data = []\n",
    "    for metric, key in zip(metrics, metric_keys):\n",
    "        data_point = {'metric': metric}\n",
    "        for impl_name, results in implementations_results.items():\n",
    "            data_point[impl_name] = results[key]\n",
    "        comparison_data.append(data_point)\n",
    "    \n",
    "    return comparison_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to implementations_comparison.png\n"
     ]
    }
   ],
   "source": [
    "implementations_results = {\n",
    "    'Vision Transformer': {\n",
    "        'accuracy': 0.86,\n",
    "        'precision': 0.86,\n",
    "        'recall': 0.86,\n",
    "        'f1_score': 0.86\n",
    "    },\n",
    "    'VGG 16': {\n",
    "        'accuracy': 0.76,\n",
    "        'precision': 0.75,\n",
    "        'recall': 0.72,\n",
    "        'f1_score': 0.74\n",
    "    },\n",
    "    'CLIP': {\n",
    "        'accuracy': 0.38,\n",
    "        'precision': 0.36,\n",
    "        'recall': 0.38,\n",
    "        'f1_score': 0.36\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the comparison plot\n",
    "plot_implementations_comparison(implementations_results, 'implementations_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
